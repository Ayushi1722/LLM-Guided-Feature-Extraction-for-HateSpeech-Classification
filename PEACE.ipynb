{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcwsjwMXWUSx",
        "outputId": "0d87b6c5-9bc7-4b87-c525-5cb7f7ea8f60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxJBNugkWeIi",
        "outputId": "feedfc45-e2ee-47a6-ada4-45c4c7696fc7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[sentencepiece]) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Data Loader"
      ],
      "metadata": {
        "id": "XQgraCgDXQF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import numpy as np\n",
        "\n",
        "import spacy\n",
        "\n",
        "\n",
        "class EncodedDataset(Dataset):\n",
        "\n",
        "  def __init__(self, input_sents: List[str],\n",
        "                input_labels: List[int],\n",
        "                input_modifers:List[List],\n",
        "                tokenizer: PreTrainedTokenizer,\n",
        "                max_sequence_length: int = None,\n",
        "                max_targets: int = 5):\n",
        "\n",
        "    self.input_sents = input_sents\n",
        "    self.input_labels = input_labels\n",
        "    self.input_modifers = input_modifers\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_sequence_length = max_sequence_length\n",
        "    self.max_targets = max_targets\n",
        "    # self.min_sequence_length = min_sequence_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_sents)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "    text = self.input_sents[index]\n",
        "    modifers = self.input_modifers[index]\n",
        "    label = self.input_labels[index]\n",
        "\n",
        "    # If we are doing some preprocessing\n",
        "    # preprocessor = PreProcess()\n",
        "\n",
        "    # senti_token = self.senti_tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
        "\n",
        "    # senti_input_ids, senti_mask_ids = torch.tensor(senti_token['input_ids']), torch.tensor(senti_token['attention_mask'])\n",
        "\n",
        "    token = self.tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
        "\n",
        "    input_ids, mask_ids = torch.tensor(token['input_ids']), torch.tensor(token['attention_mask'])\n",
        "\n",
        "    return input_ids, mask_ids, modifers, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhTGpb_3XPMS",
        "outputId": "839507b1-832d-40c7-e796-001442825d02"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom RoBERTa Model"
      ],
      "metadata": {
        "id": "i5jwEzU7Xvzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import Softmax, CrossEntropyLoss, MSELoss\n",
        "\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "class FeatureSwitchHead(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "      super().__init__()\n",
        "      self.dense = nn.Linear(config.max_sequence_length*config.num_features, config.max_sequence_length*config.num_features)\n",
        "      self.dropout = nn.Dropout(config.classifier_dropout)\n",
        "      self.out_proj = nn.Linear(config.max_sequence_length*config.num_features, config.max_sequence_length)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "\n",
        "      x = features\n",
        "      x = self.dropout(x)\n",
        "      x = self.dense(x)\n",
        "      x = torch.tanh(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.out_proj(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "class RobertaHateClassificationHead(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "      super().__init__()\n",
        "      self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "      self.dropout = nn.Dropout(config.classifier_dropout)\n",
        "      self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "      x = features[:, 0, :]\n",
        "      x = self.dropout(x)\n",
        "      x = self.dense(x)\n",
        "      x = torch.tanh(x)\n",
        "      x = self.dropout(x)\n",
        "      x = self.out_proj(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "class RobertaForHateClassification(RobertaForSequenceClassification):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config, feature_config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.soft_max = Softmax(dim=1)\n",
        "        self.switch_layer = FeatureSwitchHead(feature_config)\n",
        "        self.cls_layer = RobertaHateClassificationHead(feature_config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        aux_attention=None,\n",
        "        class_weights=None\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "\n",
        "        switch_attn = self.switch_layer(aux_attention)\n",
        "\n",
        "        sequence_output = outputs[0]*switch_attn.unsqueeze(2)\n",
        "\n",
        "        logits = self.cls_layer(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss(weight=class_weights)\n",
        "                loss_fct = loss_fct.to(device)\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        softmax_logits = self.soft_max(logits)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (softmax_logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=softmax_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "u8blIDYZXuVp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam, lr_scheduler\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from sklearn.metrics import precision_score, \\\n",
        "    recall_score, confusion_matrix, classification_report, \\\n",
        "    accuracy_score, f1_score\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "\n",
        "def train(model, train_data, train_labels, train_modifers, val_data, val_labels, val_modifers,\n",
        "          senti_model, aggre_model, tokenizer, params):\n",
        "    accumulation_steps = 4\n",
        "\n",
        "    train = EncodedDataset(input_sents=train_data,\n",
        "                    input_labels=train_labels,\n",
        "                    input_modifers=train_modifers,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_sequence_length=params.max_sequence_length)\n",
        "\n",
        "    val =  EncodedDataset(input_sents=val_data,\n",
        "                    input_labels=val_labels,\n",
        "                    input_modifers=val_modifers,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_sequence_length=params.max_sequence_length)\n",
        "\n",
        "    train_dataloader = DataLoader(train, batch_size=params.train_batch_size, shuffle=True)\n",
        "\n",
        "    val_dataloader = DataLoader(val, batch_size=params.val_batch_size)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=params.learning_rate)\n",
        "\n",
        "    earlystop_epochs = 3 # 3 consecutive epochs without validation acc increase\n",
        "\n",
        "    save_dir = \"Models/Roberta-Senti+Agg1/\"\n",
        "\n",
        "    best_validation_accuracy = 1e-5\n",
        "    best_validation_accuracy1 = 1e-5\n",
        "    without_progress = 0\n",
        "    model.zero_grad()\n",
        "    for epoch_num in range(params.epochs):\n",
        "\n",
        "      total_acc_train = 0\n",
        "      total_loss_train = 0\n",
        "      predictions = []\n",
        "      y_true = []\n",
        "      c=0\n",
        "\n",
        "      with tqdm(train_dataloader, desc=\"Training\") as loop:\n",
        "\n",
        "        for train_input, train_mask, train_modifers, train_label in loop:\n",
        "            model.train()\n",
        "            c+=1\n",
        "            train_input = train_input.to(device)\n",
        "            train_mask = train_mask.to(device)\n",
        "            train_label = train_label.to(device)\n",
        "\n",
        "            aggr_output = aggre_model(input_ids=train_input, attention_mask=train_mask, output_hidden_states = True, output_attentions=True)\n",
        "\n",
        "            senti_output = senti_model(input_ids=train_input, attention_mask=train_mask, output_hidden_states = True, output_attentions=True)\n",
        "\n",
        "            # Potential Update: Consider other layer heads (Sum/mean or specific) instead last one (-1)\n",
        "\n",
        "            aggr_att_scores = torch.sum(aggr_output.attentions[-1][0:,-1],axis=1)\n",
        "\n",
        "            senti_att_scores = torch.sum(senti_output.attentions[-1][0:,-1],axis=1)\n",
        "\n",
        "            attn_mat = torch.cat((aggr_att_scores,senti_att_scores), 1)\n",
        "\n",
        "            output = model(input_ids=train_input,\n",
        "                          attention_mask=train_mask,\n",
        "                          labels=train_label,\n",
        "                          aux_attention=attn_mat,class_weights=params.class_weights)\n",
        "\n",
        "            loss, logits = output[\"loss\"], output[\"logits\"]\n",
        "            l2_reg = torch.tensor(0., requires_grad=True)\n",
        "            for param in model.parameters():\n",
        "                l2_reg = l2_reg + torch.norm(param, 2)\n",
        "            loss = loss + (l2_reg * 0.001)\n",
        "            loss.backward()\n",
        "            if (c + 1) % accumulation_steps == 0:\n",
        "              optimizer.step()\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "            total_loss_train += loss.item()\n",
        "            acc = (logits.argmax(dim=1) == train_label).sum().item()\n",
        "            total_acc_train += acc\n",
        "\n",
        "            loop.set_postfix(loss=loss.item(), acc=acc/len(train_input))\n",
        "\n",
        "        total_acc_val = 0\n",
        "        total_loss_val = 0\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for val_input, val_mask, val_modifers, val_label in val_dataloader:\n",
        "\n",
        "                val_input = val_input.to(device)\n",
        "                val_mask = val_mask.to(device)\n",
        "\n",
        "                val_label = val_label.to(device)\n",
        "\n",
        "                aggr_output = aggre_model(input_ids=val_input, attention_mask=val_mask, output_hidden_states = True, output_attentions=True)\n",
        "\n",
        "                senti_output = senti_model(input_ids=val_input, attention_mask=val_mask, output_hidden_states = True, output_attentions=True)\n",
        "\n",
        "                aggr_att_scores = torch.sum(aggr_output.attentions[-1][0:,-1],axis=1)\n",
        "\n",
        "                senti_att_scores = torch.sum(senti_output.attentions[-1][0:,-1],axis=1)\n",
        "\n",
        "                attn_mat = torch.cat((aggr_att_scores,senti_att_scores), 1)\n",
        "\n",
        "                output = model(input_ids=val_input,\n",
        "                              attention_mask=val_mask,\n",
        "                              labels=val_label,\n",
        "                              aux_attention=attn_mat)\n",
        "\n",
        "                loss, logits = output[\"loss\"], output[\"logits\"]\n",
        "\n",
        "                acc = (logits.argmax(dim=1) == val_label).sum().item()\n",
        "\n",
        "                total_acc_val += acc\n",
        "\n",
        "                predictions.extend(logits.argmax(dim=1).detach().cpu().numpy())\n",
        "\n",
        "                y_true.extend(val_label.detach().cpu().numpy())\n",
        "\n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
        "            | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
        "        print(\"CLassification Report: \", classification_report(y_true,predictions))\n",
        "\n",
        "        ## Early Stopping Criteria\n",
        "        temp = classification_report(y_true, predictions,output_dict=True)\n",
        "        macro = pd.DataFrame(temp)['1']['f1-score']\n",
        "        m1 = pd.DataFrame(temp)['macro avg']['f1-score']\n",
        "        if macro >= best_validation_accuracy or m1>=best_validation_accuracy1:\n",
        "\n",
        "          without_progress = 0\n",
        "\n",
        "          if(macro>=best_validation_accuracy):\n",
        "            best_validation_accuracy = macro\n",
        "\n",
        "          if(m1>=best_validation_accuracy1):\n",
        "            best_validation_accuracy1 = m1\n",
        "\n",
        "          model_to_save = model\n",
        "\n",
        "          fname = \"best-model_\" + params.dataset_name+\"_\"+str(epoch_num+1)+\".pt\"\n",
        "          # torch.save(model_to_save.state_dict(), os.path.join(save_dir, fname))\n",
        "          # print(\"Saved at \",os.path.join(save_dir, fname))\n",
        "\n",
        "        else:\n",
        "\n",
        "          without_progress +=1\n",
        "\n",
        "        if without_progress >= earlystop_epochs:\n",
        "\n",
        "          print(\"Early stopping.....\")\n",
        "\n",
        "          print(\"Saving model: \", fname)\n",
        "\n",
        "          break"
      ],
      "metadata": {
        "id": "C6w77ZXUYc9r"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from types import SimpleNamespace\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "metadata": {
        "id": "RCj2ErSnZrd6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(26)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwuDa1eTZtft",
        "outputId": "67d150b2-e0f3-45c5-dcd2-56cd7c1650ab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79399abfbd90>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'Rationales_file_GAB_dataset_corrected.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "train_df = df[df['exp_split'] == 'train']\n",
        "test_df = df[df['exp_split'] == 'test']"
      ],
      "metadata": {
        "id": "K-C3XGpsZwQx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train df: \", len(train_df))\n",
        "print(\"Test_df: \", len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGznX7zkd43-",
        "outputId": "7ee7698b-c4e6-4794-db34-1c72b163f1d2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train df:  7120\n",
            "Test_df:  7120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "# del variables\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_pWnn2jeRIJ",
        "outputId": "c9800e15-b157-4772-ed0f-83f2ef511f43"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aggr_task='offensive'\n",
        "aggr_MODEL = f\"cardiffnlp/twitter-roberta-base-{aggr_task}\"\n",
        "\n",
        "aggr_model = AutoModelForSequenceClassification.from_pretrained(aggr_MODEL).to(device)\n",
        "\n",
        "latest_task='sentiment-latest'\n",
        "sentiment_MODEL = f\"cardiffnlp/twitter-roberta-base-{latest_task}\"#This is a roBERTa-base model trained on ~58M\n",
        "senti_model = AutoModelForSequenceClassification.from_pretrained(sentiment_MODEL).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSkyjMk1eeJb",
        "outputId": "02a44017-f2bb-4c18-c028-e9e68e2a6fe0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_frame = train_df\n",
        "test_frame = test_df"
      ],
      "metadata": {
        "id": "YdTXEpmQe6PX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_frame['label']), y=train_frame['label'])\n",
        "class_weights = torch.FloatTensor(class_weights)\n",
        "params = {\"max_sequence_length\": 512,\n",
        "\"learning_rate\" : 2e-5,\n",
        "\"train_batch_size\" : 2,\n",
        "\"val_batch_size\" : 2,\n",
        "\"epochs\" : 3,\n",
        "\"device\" : device,\n",
        "\"dataset_name\" : \"YouTube\",\n",
        "\"class_weights\" : class_weights,\n",
        "\"hidden_size\" : 768,\n",
        "\"num_features\" : 2,\n",
        "\"num_labels\": 2,\n",
        "\"classifier_dropout\" : 0.2\n",
        "}\n",
        "params = SimpleNamespace(**params)\n",
        "\n",
        "model_name = \"roberta-base\"\n",
        "\n",
        "base_model = RobertaForHateClassification.from_pretrained(model_name, feature_config=params).to(device)\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "train(model=base_model,\n",
        "train_data=train_frame['text'].values.tolist(),\n",
        "train_labels=train_frame['label'].values.tolist(),\n",
        "train_modifers=np.zeros(len(train_frame)).tolist(),\n",
        "val_data=test_frame['text'].values.tolist(),\n",
        "val_labels=test_frame['label'].values.tolist(),\n",
        "val_modifers=np.zeros(len(test_frame)).tolist(),\n",
        "tokenizer=tokenizer,\n",
        "senti_model = senti_model,\n",
        "aggre_model = aggr_model,\n",
        "params=params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3K2N42eevyj",
        "outputId": "eccc5594-d917-4923-e490-53cddb26669d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForHateClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'cls_layer.out_proj.weight', 'cls_layer.dense.weight', 'switch_layer.dense.weight', 'cls_layer.dense.bias', 'switch_layer.out_proj.weight', 'switch_layer.out_proj.bias', 'cls_layer.out_proj.bias', 'classifier.out_proj.bias', 'classifier.dense.bias', 'switch_layer.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3560/3560 [34:04<00:00,  1.74it/s, acc=1, loss=5.51]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Train Loss:  3.022             | Train Accuracy:  0.806             | Val Loss:  0.000             | Val Accuracy:  0.856\n",
            "CLassification Report:                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.30      0.40      1160\n",
            "           1       0.88      0.97      0.92      5960\n",
            "\n",
            "    accuracy                           0.86      7120\n",
            "   macro avg       0.75      0.63      0.66      7120\n",
            "weighted avg       0.83      0.86      0.83      7120\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3560/3560 [33:57<00:00,  1.75it/s, acc=0.5, loss=6.04]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_data, test_labels,\n",
        "          senti_tokenizer, aggre_tokenizer, test_modifiers, max_sequence_length, learning_rate, test_batch_size, device):\n",
        "\n",
        "    test = EncodedDataset(input_sents=test_data,\n",
        "                    input_labels=test_labels,\n",
        "                    input_modifers=test_modifiers,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_sequence_length=max_sequence_length)\n",
        "\n",
        "\n",
        "    test_dataloader = DataLoader(test, batch_size=test_batch_size)\n",
        "\n",
        "\n",
        "    total_acc_test = 0\n",
        "    total_loss_test = 0\n",
        "    predictions = []\n",
        "    y_true = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for test_input, test_mask, test_modifiers, test_label in test_dataloader:\n",
        "        test_input = test_input.to(device)\n",
        "        test_mask = test_mask.to(device)\n",
        "\n",
        "        # val_modifers = val_modifers.to(device)\n",
        "        test_label = test_label.to(device)\n",
        "\n",
        "        aggr_output = aggr_model(input_ids=test_input, attention_mask=test_mask, output_hidden_states = True, output_attentions=True)\n",
        "\n",
        "        senti_output = senti_model(input_ids=test_input, attention_mask=test_mask, output_hidden_states = True, output_attentions=True)\n",
        "\n",
        "        aggr_att_scores = torch.sum(aggr_output.attentions[-1][0:,-1],axis=1)\n",
        "\n",
        "        senti_att_scores = torch.sum(senti_output.attentions[-1][0:,-1],axis=1)\n",
        "\n",
        "        attn_mat = torch.cat((aggr_att_scores,senti_att_scores), 1)\n",
        "\n",
        "        output = model(input_ids=test_input,\n",
        "                      attention_mask=test_mask,\n",
        "                      labels=test_label,\n",
        "                      aux_attention=attn_mat)\n",
        "\n",
        "        loss, logits = output[\"loss\"], output[\"logits\"]\n",
        "\n",
        "        acc = (logits.argmax(dim=1) == test_label).sum().item()\n",
        "\n",
        "        predictions.extend(logits.argmax(dim=1).detach().cpu().numpy())\n",
        "\n",
        "        y_true.extend(test_label.detach().cpu().numpy())\n",
        "      return predictions,y_true"
      ],
      "metadata": {
        "id": "xBumCFVIY2or"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds,trues = evaluate(model=base_model, test_data = test_frame['text'].values.tolist(), test_labels=test_frame['label'].values.tolist(),\n",
        "senti_tokenizer=senti_model, aggre_tokenizer=aggr_model,test_modifiers=np.zeros(len(test_frame)), max_sequence_length=512, learning_rate=1e-5, test_batch_size=2, device=device)\n",
        "# print(f1)\n",
        "print(\"*\"*100)\n",
        "print(classification_report(trues,preds))"
      ],
      "metadata": {
        "id": "Yc7qBYhNZAXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd24e1bd-0438-4f9b-b341-61c559f6785a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************************************************************************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.79      0.80      2841\n",
            "           1       0.53      0.55      0.54      1187\n",
            "\n",
            "    accuracy                           0.72      4028\n",
            "   macro avg       0.67      0.67      0.67      4028\n",
            "weighted avg       0.72      0.72      0.72      4028\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xhDWbtAMf-fZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}