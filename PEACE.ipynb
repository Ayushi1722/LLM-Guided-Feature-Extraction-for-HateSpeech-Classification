{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RcwsjwMXWUSx",
    "outputId": "0d87b6c5-9bc7-4b87-c525-5cb7f7ea8f60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxJBNugkWeIi",
    "outputId": "feedfc45-e2ee-47a6-ada4-45c4c7696fc7"
   },
   "outputs": [],
   "source": [
    "!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQgraCgDXQF7"
   },
   "source": [
    "# Custom Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AhTGpb_3XPMS",
    "outputId": "839507b1-832d-40c7-e796-001442825d02"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/local/ASUAD/abhatt43/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/local/ASUAD/abhatt43/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "class EncodedDataset(Dataset):\n",
    "\n",
    "    def __init__(self, input_sents: List[str],\n",
    "                input_labels: List[int],\n",
    "                input_modifers:List[List],\n",
    "                tokenizer: PreTrainedTokenizer,\n",
    "                max_sequence_length: int = None,\n",
    "                max_targets: int = 5):\n",
    "\n",
    "        self.input_sents = input_sents\n",
    "        self.input_labels = input_labels\n",
    "        self.input_modifers = input_modifers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.max_targets = max_targets\n",
    "        # self.min_sequence_length = min_sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_sents)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        text = self.input_sents[index]\n",
    "        modifers = self.input_modifers[index]\n",
    "        label = self.input_labels[index]\n",
    "\n",
    "        # If we are doing some preprocessing\n",
    "        # preprocessor = PreProcess()\n",
    "\n",
    "        # senti_token = self.senti_tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
    "\n",
    "        # senti_input_ids, senti_mask_ids = torch.tensor(senti_token['input_ids']), torch.tensor(senti_token['attention_mask'])\n",
    "\n",
    "        token = self.tokenizer(text, padding='max_length', max_length= self.max_sequence_length, truncation=True)\n",
    "\n",
    "        input_ids, mask_ids = torch.tensor(token['input_ids']), torch.tensor(token['attention_mask'])\n",
    "\n",
    "        return input_ids, mask_ids, modifers, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5jwEzU7Xvzq"
   },
   "source": [
    "# Custom RoBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "u8blIDYZXuVp"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Softmax, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "class FeatureSwitchHead(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.max_sequence_length*config.num_features, config.max_sequence_length*config.num_features)\n",
    "        self.dropout = nn.Dropout(config.classifier_dropout)\n",
    "        self.out_proj = nn.Linear(config.max_sequence_length*config.num_features, config.max_sequence_length)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class RobertaHateClassificationHead(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.classifier_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RobertaForHateClassification(RobertaForSequenceClassification):\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def __init__(self, config, feature_config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.soft_max = Softmax(dim=1)\n",
    "        self.switch_layer = FeatureSwitchHead(feature_config)\n",
    "        self.cls_layer = RobertaHateClassificationHead(feature_config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        aux_attention=None,\n",
    "        class_weights=None\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
    "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "\n",
    "        switch_attn = self.switch_layer(aux_attention)\n",
    "\n",
    "        sequence_output = outputs[0]*switch_attn.unsqueeze(2)\n",
    "\n",
    "        logits = self.cls_layer(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss(weight=class_weights)\n",
    "                loss_fct = loss_fct.to(device)\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        softmax_logits = self.soft_max(logits)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (softmax_logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=softmax_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "C6w77ZXUYc9r"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "def train(model, train_data, train_labels, train_modifers, val_data, val_labels, val_modifers,\n",
    "          senti_model, aggre_model, tokenizer, params):\n",
    "    accumulation_steps = 4\n",
    "\n",
    "    train = EncodedDataset(input_sents=train_data,\n",
    "                    input_labels=train_labels,\n",
    "                    input_modifers=train_modifers,\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_sequence_length=params.max_sequence_length)\n",
    "\n",
    "    val =  EncodedDataset(input_sents=val_data,\n",
    "                    input_labels=val_labels,\n",
    "                    input_modifers=val_modifers,\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_sequence_length=params.max_sequence_length)\n",
    "\n",
    "    train_dataloader = DataLoader(train, batch_size=params.train_batch_size, shuffle=True)\n",
    "\n",
    "    val_dataloader = DataLoader(val, batch_size=params.val_batch_size)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=params.learning_rate)\n",
    "\n",
    "    earlystop_epochs = 3 # 3 consecutive epochs without validation acc increase\n",
    "\n",
    "    save_dir = \"/home/local/ASUAD/abhatt43/Projects/HateSpeech/notebooks/\"\n",
    "\n",
    "    best_validation_accuracy = 1e-5\n",
    "    best_validation_accuracy1 = 1e-5\n",
    "    without_progress = 0\n",
    "    model.zero_grad()\n",
    "    for epoch_num in range(params.epochs):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        predictions = []\n",
    "        y_true = []\n",
    "        c=0\n",
    "\n",
    "        with tqdm(train_dataloader, desc=\"Training\") as loop:\n",
    "\n",
    "            for train_input, train_mask, train_modifers, train_label in loop:\n",
    "                model.train()\n",
    "                c+=1\n",
    "                train_input = train_input.to(device)\n",
    "                train_mask = train_mask.to(device)\n",
    "                train_label = train_label.to(device)\n",
    "\n",
    "                aggr_output = aggre_model(input_ids=train_input, attention_mask=train_mask, output_hidden_states = True, output_attentions=True)\n",
    "\n",
    "                senti_output = senti_model(input_ids=train_input, attention_mask=train_mask, output_hidden_states = True, output_attentions=True)\n",
    "\n",
    "                # Potential Update: Consider other layer heads (Sum/mean or specific) instead last one (-1)\n",
    "\n",
    "                aggr_att_scores = torch.sum(aggr_output.attentions[-1][0:,-1],axis=1)\n",
    "\n",
    "                senti_att_scores = torch.sum(senti_output.attentions[-1][0:,-1],axis=1)\n",
    "\n",
    "                attn_mat = torch.cat((aggr_att_scores,senti_att_scores), 1)\n",
    "\n",
    "                output = model(input_ids=train_input,\n",
    "                              attention_mask=train_mask,\n",
    "                              labels=train_label,\n",
    "                              aux_attention=attn_mat,class_weights=params.class_weights)\n",
    "\n",
    "                loss, logits = output[\"loss\"], output[\"logits\"]\n",
    "                l2_reg = torch.tensor(0., requires_grad=True)\n",
    "                for param in model.parameters():\n",
    "                    l2_reg = l2_reg + torch.norm(param, 2)\n",
    "                loss = loss + (l2_reg * 0.001)\n",
    "                loss.backward()\n",
    "                if (c + 1) % accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                total_loss_train += loss.item()\n",
    "                acc = (logits.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                loop.set_postfix(loss=loss.item(), acc=acc/len(train_input))\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_mask, val_modifers, val_label in val_dataloader:\n",
    "\n",
    "                val_input = val_input.to(device)\n",
    "                val_mask = val_mask.to(device)\n",
    "\n",
    "                val_label = val_label.to(device)\n",
    "\n",
    "                aggr_output = aggre_model(input_ids=val_input, attention_mask=val_mask, output_hidden_states = True, output_attentions=True)\n",
    "\n",
    "                senti_output = senti_model(input_ids=val_input, attention_mask=val_mask, output_hidden_states = True, output_attentions=True)\n",
    "\n",
    "                aggr_att_scores = torch.sum(aggr_output.attentions[-1][0:,-1],axis=1)\n",
    "\n",
    "                senti_att_scores = torch.sum(senti_output.attentions[-1][0:,-1],axis=1)\n",
    "\n",
    "                attn_mat = torch.cat((aggr_att_scores,senti_att_scores), 1)\n",
    "\n",
    "                output = model(input_ids=val_input,\n",
    "                              attention_mask=val_mask,\n",
    "                              labels=val_label,\n",
    "                              aux_attention=attn_mat)\n",
    "\n",
    "                loss, logits = output[\"loss\"], output[\"logits\"]\n",
    "\n",
    "                acc = (logits.argmax(dim=1) == val_label).sum().item()\n",
    "\n",
    "                total_acc_val += acc\n",
    "\n",
    "                predictions.extend(logits.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "                y_true.extend(val_label.detach().cpu().numpy())\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "        print(\"CLassification Report: \", classification_report(y_true,predictions))\n",
    "\n",
    "        ## Early Stopping Criteria\n",
    "        temp = classification_report(y_true, predictions,output_dict=True)\n",
    "        macro = pd.DataFrame(temp)['1']['f1-score']\n",
    "        m1 = pd.DataFrame(temp)['macro avg']['f1-score']\n",
    "        if macro >= best_validation_accuracy or m1>=best_validation_accuracy1:\n",
    "\n",
    "            without_progress = 0\n",
    "\n",
    "            if(macro>=best_validation_accuracy):\n",
    "                best_validation_accuracy = macro\n",
    "\n",
    "            if(m1>=best_validation_accuracy1):\n",
    "                best_validation_accuracy1 = m1\n",
    "\n",
    "            model_to_save = model\n",
    "\n",
    "            fname = \"best-model_\" + params.dataset_name+\"_\"+str(epoch_num+1)+\".pt\"\n",
    "          # torch.save(model_to_save.state_dict(), os.path.join(save_dir, fname))\n",
    "          # print(\"Saved at \",os.path.join(save_dir, fname))\n",
    "\n",
    "        else:\n",
    "\n",
    "            without_progress +=1\n",
    "\n",
    "        if without_progress >= earlystop_epochs:\n",
    "\n",
    "            print(\"Early stopping.....\")\n",
    "\n",
    "            print(\"Saving model: \", fname)\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RCj2ErSnZrd6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from types import SimpleNamespace\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jwuDa1eTZtft",
    "outputId": "67d150b2-e0f3-45c5-dcd2-56cd7c1650ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa7a68c4f48>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "K-C3XGpsZwQx"
   },
   "outputs": [],
   "source": [
    "file_path = '/home/local/ASUAD/abhatt43/Projects/HateSpeech/data/Rationales_file_REDDIT_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "train_df = df[df['exp_split'] == 'train']\n",
    "test_df = df[df['exp_split'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IGznX7zkd43-",
    "outputId": "7ee7698b-c4e6-4794-db34-1c72b163f1d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train df:  29731\n",
      "Test_df:  7433\n"
     ]
    }
   ],
   "source": [
    "print(\"Train df: \", len(train_df))\n",
    "print(\"Test_df: \", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_pWnn2jeRIJ",
    "outputId": "c9800e15-b157-4772-ed0f-83f2ef511f43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "# del variables\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VSkyjMk1eeJb",
    "outputId": "02a44017-f2bb-4c18-c028-e9e68e2a6fe0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "aggr_task='offensive'\n",
    "aggr_MODEL = f\"cardiffnlp/twitter-roberta-base-{aggr_task}\"\n",
    "\n",
    "aggr_model = AutoModelForSequenceClassification.from_pretrained(aggr_MODEL).to(device)\n",
    "\n",
    "latest_task='sentiment-latest'\n",
    "sentiment_MODEL = f\"cardiffnlp/twitter-roberta-base-{latest_task}\"#This is a roBERTa-base model trained on ~58M\n",
    "senti_model = AutoModelForSequenceClassification.from_pretrained(sentiment_MODEL).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_text, val_texts, train_labels, val_labels = train_test_split(train_df['text'].tolist(),train_df['label'].tolist(), test_size = 0.2, random_state = 42)\n",
    "# splitting train_df into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "YdTXEpmQe6PX"
   },
   "outputs": [],
   "source": [
    "# train_frame = train_df\n",
    "test_frame = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3K2N42eevyj",
    "outputId": "eccc5594-d917-4923-e490-53cddb26669d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForHateClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForHateClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForHateClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForHateClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'switch_layer.dense.weight', 'switch_layer.dense.bias', 'switch_layer.out_proj.weight', 'switch_layer.out_proj.bias', 'cls_layer.dense.weight', 'cls_layer.dense.bias', 'cls_layer.out_proj.weight', 'cls_layer.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training:   3%|â–Ž         | 357/11892 [01:31<48:04,  4.00it/s, acc=0.5, loss=6.28]"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights = torch.FloatTensor(class_weights)\n",
    "params = {\"max_sequence_length\": 512,\n",
    "\"learning_rate\" : 2e-5,\n",
    "\"train_batch_size\" : 2,\n",
    "\"val_batch_size\" : 2,\n",
    "\"epochs\" : 3,\n",
    "\"device\" : device,\n",
    "\"dataset_name\" : \"Reddit\",\n",
    "\"class_weights\" : class_weights,\n",
    "\"hidden_size\" : 768,\n",
    "\"num_features\" : 2,\n",
    "\"num_labels\": 2,\n",
    "\"classifier_dropout\" : 0.2\n",
    "}\n",
    "params = SimpleNamespace(**params)\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "\n",
    "base_model = RobertaForHateClassification.from_pretrained(model_name, feature_config=params).to(device)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "train(model=base_model,\n",
    "# train_data=train_frame['text'].values.tolist(),\n",
    "# train_labels=train_frame['label'].values.tolist(),\n",
    "train_data=train_text,\n",
    "train_labels=train_labels,\n",
    "train_modifers=np.zeros(len(train_text)).tolist(),\n",
    "# val_data=test_frame['text'].values.tolist(),\n",
    "# val_labels=test_frame['label'].values.tolist(),\n",
    "val_data=val_texts,\n",
    "val_labels=val_labels,\n",
    "val_modifers=np.zeros(len(test_frame)).tolist(),\n",
    "tokenizer=tokenizer,\n",
    "senti_model = senti_model,\n",
    "aggre_model = aggr_model,\n",
    "params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBumCFVIY2or"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_data, test_labels,\n",
    "          senti_tokenizer, aggre_tokenizer, test_modifiers, max_sequence_length, learning_rate, test_batch_size, device):\n",
    "\n",
    "    test = EncodedDataset(input_sents=test_data,\n",
    "                    input_labels=test_labels,\n",
    "                    input_modifers=test_modifiers,\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_sequence_length=max_sequence_length)\n",
    "\n",
    "\n",
    "    test_dataloader = DataLoader(test, batch_size=test_batch_size)\n",
    "\n",
    "\n",
    "    total_acc_test = 0\n",
    "    total_loss_test = 0\n",
    "    predictions = []\n",
    "    y_true = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_mask, test_modifiers, test_label in test_dataloader:\n",
    "        test_input = test_input.to(device)\n",
    "        test_mask = test_mask.to(device)\n",
    "\n",
    "        # val_modifers = val_modifers.to(device)\n",
    "        test_label = test_label.to(device)\n",
    "\n",
    "        aggr_output = aggr_model(input_ids=test_input, attention_mask=test_mask, output_hidden_states = True, output_attentions=True)\n",
    "\n",
    "        senti_output = senti_model(input_ids=test_input, attention_mask=test_mask, output_hidden_states = True, output_attentions=True)\n",
    "\n",
    "        aggr_att_scores = torch.sum(aggr_output.attentions[-1][0:,-1],axis=1)\n",
    "\n",
    "        senti_att_scores = torch.sum(senti_output.attentions[-1][0:,-1],axis=1)\n",
    "\n",
    "        attn_mat = torch.cat((aggr_att_scores,senti_att_scores), 1)\n",
    "\n",
    "        output = model(input_ids=test_input,\n",
    "                      attention_mask=test_mask,\n",
    "                      labels=test_label,\n",
    "                      aux_attention=attn_mat)\n",
    "\n",
    "        loss, logits = output[\"loss\"], output[\"logits\"]\n",
    "\n",
    "        acc = (logits.argmax(dim=1) == test_label).sum().item()\n",
    "\n",
    "        predictions.extend(logits.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "        y_true.extend(test_label.detach().cpu().numpy())\n",
    "    return predictions,y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yc7qBYhNZAXH",
    "outputId": "cd24e1bd-0438-4f9b-b341-61c559f6785a"
   },
   "outputs": [],
   "source": [
    "preds,trues = evaluate(model=base_model, test_data = test_frame['text'].values.tolist(), test_labels=test_frame['label'].values.tolist(),\n",
    "senti_tokenizer=senti_model, aggre_tokenizer=aggr_model,test_modifiers=np.zeros(len(test_frame)), max_sequence_length=512, learning_rate=1e-5, test_batch_size=2, device=device)\n",
    "# print(f1)\n",
    "print(\"*\"*100)\n",
    "print(classification_report(trues,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhDWbtAMf-fZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:contrastive]",
   "language": "python",
   "name": "conda-env-contrastive-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
