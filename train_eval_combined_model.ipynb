{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayushi1722/LLM-Guided-Feature-Extraction-for-HateSpeech-Classification/blob/main/train_eval_combined_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okZsHbjpI4AZ",
        "outputId": "fc0d94bf-f52b-4ffb-932a-2961d00fce2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.23.1 in /usr/local/lib/python3.10/dist-packages (4.23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.23.1) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.23.1) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1) (2023.11.17)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers==4.23.1\n",
        "! pip install torch torchvision\n",
        "! pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5Mzsr3mGI_eB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import AdamW\n",
        "# from GPUtil import showUtilization as gpu_usage\n",
        "# from numba import cuda\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AAsC5Cq-owb",
        "outputId": "2c384a10-705e-4135-9314-4d1b330a9b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available:  True\n"
          ]
        }
      ],
      "source": [
        "print(\"CUDA Available: \", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "03T-h8ACYt7v"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7oidRv7Yt7v",
        "outputId": "a9e8ec2a-4e47-4e3e-9637-73f310deac1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## set dataset here\n",
        "# options = [\"gab\", \"twitter\", \"reddit\", \"youtube\"]\n",
        "\n",
        "\n",
        "dataset = \"youtube\"\n",
        "\n",
        "file_map = {\n",
        "    \"gab\": '/content/Rationales_file_GAB_dataset_corrected.csv',\n",
        "    \"twitter\": '/content/Rationales_file_TWITTER_dataset.csv',\n",
        "    \"reddit\": '/content/Rationales_file_REDDIT_dataset.csv',\n",
        "    \"youtube\": '/content/Rationales_file_YOUTUBE_dataset.csv'\n",
        "}"
      ],
      "metadata": {
        "id": "fI0aXm_AnSGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A0Yq2uSrUDhP"
      },
      "outputs": [],
      "source": [
        "file_path = file_map[dataset]\n",
        "df = pd.read_csv(file_path)\n",
        "train_df = df[df['exp_split'] == 'train']\n",
        "test_df = df[df['exp_split'] == 'test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxLD6WbYbKRB",
        "outputId": "ed8d1d81-67ec-404d-c9b7-cb2149b32204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train df:  1024\n",
            "Test_df:  4028\n"
          ]
        }
      ],
      "source": [
        "print(\"Train df: \", len(train_df))\n",
        "print(\"Test_df: \", len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMG59qBiYt70",
        "outputId": "40f2583a-b72c-438f-ea36-221cc3191255"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import gc\n",
        "# del variables\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaA7GHci-ows"
      },
      "source": [
        "# Dataset and Model Classes and Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(\"GroNLP/hateBERT\") ## need this for tokenizing the input text in data loader\n",
        "model_bert = AutoModel.from_pretrained(bert_model_name)\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(bert_model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cjyEVhfA2ms",
        "outputId": "6d533de8-f880-4429-a67d-ed3405fb0e65"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VcDZeNVeIAVh"
      },
      "outputs": [],
      "source": [
        "class AdditionalCustomDataset(Dataset):\n",
        "  def __init__(self, texts, labels, additional_texts, tokenizer, bert_tokenizer, max_length):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.additional_texts = additional_texts\n",
        "    self.tokenizer = tokenizer\n",
        "    self.bert_tokenizer = bert_tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    texts = self.texts[idx]\n",
        "    additional_texts = self.additional_texts[idx]\n",
        "    labels = self.labels[idx]\n",
        "    encoding = self.tokenizer(texts, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "    additional_encoding = self.bert_tokenizer(additional_texts, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "    original_input_ids = encoding['input_ids'].squeeze()\n",
        "    additional_input_ids = additional_encoding['input_ids'].squeeze()\n",
        "    input_ids = torch.cat((encoding[\"input_ids\"], additional_encoding[\"input_ids\"]), dim=1)\n",
        "    original_attention_mask = encoding['attention_mask'].squeeze()\n",
        "    additional_attention_mask = additional_encoding['attention_mask'].squeeze()\n",
        "    attention_mask = torch.cat((encoding[\"attention_mask\"], additional_encoding[\"attention_mask\"]), dim=1)\n",
        "    labels = labels\n",
        "    return original_input_ids, original_attention_mask, additional_input_ids, additional_attention_mask, labels\n",
        "    # return input_ids, attention_mask, labels\n",
        "    # return encoding, additional_encoding, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4ab71_wZH46w"
      },
      "outputs": [],
      "source": [
        "#Splitting training and validation testing split to test accuracy\n",
        "train_text, val_texts, train_labels, val_labels = train_test_split(train_df['text'].tolist(),train_df['label'].tolist(), test_size = 0.2, random_state = 42)\n",
        "add_train_text, add_val_texts, add_train_labels, add_val_labels = train_test_split(train_df['ChatGPT_Rationales'].tolist(),train_df['label'].tolist(), test_size = 0.2, random_state = 42)\n",
        "# train_text, add_train_texts, val_texts, train_labels, add_train_labels, val_labels = train_test_split(train_df['text'].tolist(), train_df['ChatGPT Response'], train_df['label'].tolist(), test_size = 0.2, random_state = 42)\n",
        "\n",
        "## Creating a CustomDataset\n",
        "train_dataset = AdditionalCustomDataset(train_text, train_labels, add_train_text, tokenizer, tokenizer_bert, max_length = 512)\n",
        "val_dataset = AdditionalCustomDataset(val_texts, val_labels, add_val_texts, tokenizer, tokenizer_bert, max_length = 512)\n",
        "\n",
        "#Creating dataloader object to train the model\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWPuFoa0-owt"
      },
      "source": [
        "# Our Concatenated Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0c0yGUoE-owu"
      },
      "outputs": [],
      "source": [
        "class ProjectionMLP(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(ProjectionMLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, output_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(output_size, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "q059CC5X-owu"
      },
      "outputs": [],
      "source": [
        "class ConcatModel(nn.Module):\n",
        "    def __init__(self, hatebert_model, additional_model, projection_mlp, freeze_additional_model=True):\n",
        "        super(ConcatModel, self).__init__()\n",
        "        self.hatebert_model = hatebert_model\n",
        "        self.additional_model = additional_model\n",
        "        self.projection_mlp = projection_mlp\n",
        "\n",
        "        if freeze_additional_model:\n",
        "            for param in self.additional_model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, additional_input_ids, additional_attention_mask):\n",
        "        # Forward pass through the HateBERT model\n",
        "        hatebert_outputs = self.hatebert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hatebert_embeddings = hatebert_outputs.last_hidden_state[:, 0, :]  # Assuming [CLS] token representation\n",
        "\n",
        "        # Forward pass through the Additional Model\n",
        "        additional_outputs = self.additional_model(input_ids=additional_input_ids, attention_mask=additional_attention_mask)\n",
        "        additional_embeddings = additional_outputs.last_hidden_state[:, 0, :]  # Assuming [CLS] token representation\n",
        "\n",
        "        # Concatenate the embeddings\n",
        "        concatenated_embeddings = torch.cat((hatebert_embeddings, additional_embeddings), dim=1)\n",
        "        # print(\"Size of concatenated embeddings:\", concatenated_embeddings.size())\n",
        "\n",
        "        # Project concatenated embeddings\n",
        "        projected_embeddings = self.projection_mlp(concatenated_embeddings)\n",
        "\n",
        "        return projected_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jOzmk_Pq-owu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31633847-cea8-421b-f557-136c4f110b54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at GroNLP/HateBERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "hatebert_model = BertModel.from_pretrained(\"GroNLP/HateBERT\")\n",
        "additional_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "projection_mlp = ProjectionMLP(input_size=1536, output_size=512)\n",
        "\n",
        "concat_model = ConcatModel(hatebert_model=hatebert_model, additional_model=additional_model, projection_mlp=projection_mlp, freeze_additional_model=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_Eeyjii-owu"
      },
      "outputs": [],
      "source": [
        "concat_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "E546ycZ_-owu"
      },
      "outputs": [],
      "source": [
        "concat_model = concat_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(concat_model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_LO6sSxFJm-",
        "outputId": "8880425e-3807-4713-a5f9-7ab6371fba9c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVMDsv63-owu"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 3  # 5 is too much, overfits\n",
        "for epoch in range(num_epochs):\n",
        "    concat_model.train()\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracy = 0\n",
        "    train_epoch_size = 0\n",
        "\n",
        "    with tqdm(train_dataloader, desc=f'Epoch {epoch + 1}', dynamic_ncols=True) as loop:\n",
        "        for batch in loop:\n",
        "            input_ids, attention_mask, additional_input_ids, additional_attention_mask, labels = batch\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                additional_input_ids = additional_input_ids.to(device)\n",
        "                additional_attention_mask = additional_attention_mask.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "            # Forward pass through the ConcatModel\n",
        "            optimizer.zero_grad()\n",
        "            outputs = concat_model(input_ids=input_ids, attention_mask=attention_mask, additional_input_ids=additional_input_ids, additional_attention_mask=additional_attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss = criterion(outputs.view(-1, 2), labels.view(-1)) # 2 is number of labels\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            # Update accuracy and epoch size\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            train_accuracy += (predictions == labels).sum().item()\n",
        "            train_epoch_size += len(labels)\n",
        "\n",
        "            # Update tqdm progress bar with set_postfix\n",
        "            loop.set_postfix(loss=loss.item(), accuracy=train_accuracy / train_epoch_size)\n",
        "\n",
        "    # Evaluation on the validation set\n",
        "    concat_model.eval()\n",
        "\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad(), tqdm(val_dataloader, desc='Validation', dynamic_ncols=True) as loop:\n",
        "        for batch in loop:\n",
        "            input_ids, attention_mask, additional_input_ids, additional_attention_mask, labels = batch\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                additional_input_ids = additional_input_ids.to(device)\n",
        "                additional_attention_mask = additional_attention_mask.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "            # Forward pass through the ConcatModel\n",
        "            outputs = concat_model(input_ids=input_ids, attention_mask=attention_mask, additional_input_ids=additional_input_ids, additional_attention_mask=additional_attention_mask)\n",
        "            sm = nn.Softmax(dim=1)\n",
        "            predictions = torch.argmax(sm(outputs), dim=1)\n",
        "            # print(\"prediction: \", predictions)\n",
        "            # sm = nn.Softmax(dim=1)\n",
        "            # predictions2 = sm(outputs)\n",
        "            # print(\"prediction2: \", predictions2)\n",
        "            val_predictions.extend(predictions.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate and print validation accuracy\n",
        "    accuracy = accuracy_score(val_predictions, val_labels)\n",
        "    print(f\"Epoch {epoch + 1}: Validation Accuracy: {accuracy:.4f}, Avg. Train Loss: {sum(train_losses) / len(train_losses):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-VdTWee--owv"
      },
      "outputs": [],
      "source": [
        "torch.save(concat_model, f'combined_model_fine_tuned_{dataset}.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation on Test Set"
      ],
      "metadata": {
        "id": "Axy6hN6Jo9A-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ktrQxsKF-owv"
      },
      "outputs": [],
      "source": [
        "test_texts = test_df['text'].tolist()\n",
        "add_test_texts = test_df['ChatGPT_Rationales'].tolist()\n",
        "test_labels = test_df['label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FVHxBQ-4-owv"
      },
      "outputs": [],
      "source": [
        "test_dataset = AdditionalCustomDataset(train_text, train_labels, add_train_text, tokenizer, tokenizer_bert, max_length = 512)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "u6NwVxGk-owv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77decb93-da3c-4e29-ae9c-6bf0941c3ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of test dataset: 0.9938949938949939\n"
          ]
        }
      ],
      "source": [
        "concat_model.eval()\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids, attention_mask, additional_input_ids, additional_attention_mask, labels = batch\n",
        "        if torch.cuda.is_available():\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                additional_input_ids = additional_input_ids.to(device)\n",
        "                additional_attention_mask = additional_attention_mask.to(device)\n",
        "                labels = labels.to(device)\n",
        "        outputs = concat_model(input_ids=input_ids, attention_mask=attention_mask, additional_input_ids=additional_input_ids, additional_attention_mask=additional_attention_mask)\n",
        "        predictions = torch.argmax(outputs, dim=1)\n",
        "        test_predictions.extend(predictions.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(test_predictions, test_labels)\n",
        "\n",
        "print(\"Accuracy of test dataset:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "z8sKAtAE-owz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddfca21c-beb9-4aa1-ee57-6cd3788123a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del concat_model"
      ],
      "metadata": {
        "id": "_UqNzyniRTl5"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OPWcf8W-RU1U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}