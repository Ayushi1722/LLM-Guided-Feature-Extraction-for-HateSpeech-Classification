{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayushi1722/LLM-Guided-Feature-Extraction-for-HateSpeech-Classification/blob/main/train_eval_naive_hatebert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers==4.23.1\n",
        "! pip install torch torchvision\n",
        "! pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fJZWjqypSll",
        "outputId": "b2d89986-ecab-4baa-ae22-b8689baa9b83"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.23.1 in /usr/local/lib/python3.10/dist-packages (4.23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.1) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.23.1) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.23.1) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.23.1) (2023.11.17)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5Mzsr3mGI_eB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import AdamW\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BJsGQFxrovEL",
        "outputId": "a91c2c31-6e5c-43c4-8d7b-a46dcd259ffd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available:  True\n"
          ]
        }
      ],
      "source": [
        "print(\"CUDA Available: \", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "03T-h8ACYt7v"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7oidRv7Yt7v",
        "outputId": "49500e8b-aa55-4533-cbd7-ec31d5c6d721"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## set dataset here\n",
        "# options = [\"gab\", \"twitter\", \"reddit\", \"youtube\"]\n",
        "\n",
        "\n",
        "dataset = \"youtube\"\n",
        "\n",
        "file_map = {\n",
        "    \"gab\": '/content/Rationales_file_GAB_dataset_corrected.csv',\n",
        "    \"twitter\": '/content/Rationales_file_TWITTER_dataset.csv',\n",
        "    \"reddit\": '/content/Rationales_file_REDDIT_dataset.csv',\n",
        "    \"youtube\": '/content/Rationales_file_YOUTUBE_dataset.csv'\n",
        "}"
      ],
      "metadata": {
        "id": "54eWRqpW2AwE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Vu3QBnaoYt7w"
      },
      "outputs": [],
      "source": [
        "file_path = file_map[dataset]\n",
        "df = pd.read_csv(file_path)\n",
        "train_df = df[df['exp_split'] == 'train']\n",
        "test_df = df[df['exp_split'] == 'test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxLD6WbYbKRB",
        "outputId": "c226efa0-47ea-4be0-b77f-32b8fa307712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train df:  1024\n",
            "Test_df:  4028\n"
          ]
        }
      ],
      "source": [
        "print(\"Train df: \", len(train_df))\n",
        "print(\"Test_df: \", len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nieCECDxcKlZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a9139e-8b45-428e-e03c-311f7f263260"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import gc\n",
        "# del variables\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFRfebXdrPlg",
        "outputId": "4688ad2b-7c1b-4a95-9ef2-343f189f4a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at GroNLP/hateBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_name = 'GroNLP/HateBERT'\n",
        "tokenizer = BertTokenizer.from_pretrained(\"GroNLP/hateBERT\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"GroNLP/hateBERT\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "giR4SxPbH_jp"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, texts, labels, tokenizer, max_length):\n",
        "    self.texts = texts\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    texts = self.texts[idx]\n",
        "    labels = self.labels[idx]\n",
        "    encoding = self.tokenizer(texts, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "    # input_ids, mask_ids = torch.tensor(encoding['input_ids']), torch.tensor(encoding['attention_mask'])\n",
        "    input_ids = encoding['input_ids'].squeeze()\n",
        "    attention_mask = encoding['attention_mask'].squeeze()\n",
        "    labels = labels\n",
        "    return input_ids, attention_mask, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGFYvQiuzFfJ",
        "outputId": "88139079-9199-4757-8ba7-e7f139aa0f2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters for tuning model initially. Let's see, we will change if required.\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NbjfxJqMc7Yt"
      },
      "outputs": [],
      "source": [
        "#Splitting training and validation testing split to test accuracy\n",
        "train_text, val_texts, train_labels, val_labels = train_test_split(train_df['text'].tolist(),train_df['label'].tolist(), test_size = 0.2, random_state = 42)\n",
        "train_dataset = CustomDataset(train_text, train_labels, tokenizer, max_length = 512)\n",
        "val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_length = 512)\n",
        "\n",
        "#Creating dataloader object to train the model\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2PQbpEJ3Yt70"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMG59qBiYt70",
        "outputId": "54edd02e-e418-494a-dd23-9d32cd900bac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import gc\n",
        "# del variables\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracy = 0\n",
        "    train_epoch_size = 0\n",
        "\n",
        "    with tqdm(train_dataloader, desc=f'Epoch {epoch + 1}', dynamic_ncols=True) as loop:\n",
        "      for batch in loop:\n",
        "          input_ids, mask_ids, labels = batch\n",
        "          if torch.cuda.is_available():\n",
        "              input_ids = input_ids.to(device)\n",
        "              mask_ids = mask_ids.to(device)\n",
        "              labels = labels.to(device)\n",
        "          # optimizer.grad()\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(input_ids=input_ids, attention_mask=mask_ids, labels=labels)\n",
        "          loss = outputs.loss\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_losses.append(loss.item())\n",
        "\n",
        "          # Update accuracy and epoch size\n",
        "          predictions = torch.argmax(outputs.logits, dim=1)\n",
        "          train_accuracy += (predictions == labels).sum().item()\n",
        "          train_epoch_size += len(labels)\n",
        "\n",
        "          # Update tqdm progress bar with set_postfix\n",
        "          loop.set_postfix(loss=loss.item(), accuracy=train_accuracy / train_epoch_size)\n",
        "\n",
        "\n",
        "    # Evaluating on Validation task\n",
        "    model.eval()\n",
        "\n",
        "    val_predictions = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids, mask_ids, labels = batch\n",
        "            if torch.cuda.is_available():\n",
        "                input_ids = input_ids.to(device)\n",
        "                mask_ids = mask_ids.to(device)\n",
        "                labels = labels.to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=mask_ids)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            val_predictions.extend(predictions.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(val_predictions, val_labels)\n",
        "    print(f\"Epoch {epoch + 1}: Validation Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx9zCWKarK13",
        "outputId": "5d614b7a-c57d-4102-ff48-f25729b5eb63"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 410/410 [01:27<00:00,  4.68it/s, accuracy=0.756, loss=2.22]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Validation Accuracy: 0.8293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 410/410 [01:29<00:00,  4.58it/s, accuracy=0.945, loss=0.0149]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Validation Accuracy: 0.8341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 410/410 [01:29<00:00,  4.57it/s, accuracy=0.974, loss=0.00323]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Validation Accuracy: 0.8683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EabT3TLIzSx3"
      },
      "outputs": [],
      "source": [
        "torch.save(model, f'fine_tuned_naive_hatebert_{dataset}.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVBoJnMrYt72"
      },
      "source": [
        "# Testing on Tests Dataset for accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uA71SvcgzS0X"
      },
      "outputs": [],
      "source": [
        "test_texts = test_df['text'].tolist()\n",
        "test_labels = test_df['label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "P8qdQ_siYt73"
      },
      "outputs": [],
      "source": [
        "test_dataset = CustomDataset(test_texts, test_labels, tokenizer, max_length = 512)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dzWaSlHgYt74"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids, mask_ids, labels = batch\n",
        "        if torch.cuda.is_available():\n",
        "                input_ids = input_ids.to(device)\n",
        "                mask_ids = mask_ids.to(device)\n",
        "                labels = labels.to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=mask_ids)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        test_predictions.extend(predictions.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(test_predictions, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PR_tY9NYYt74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6fd0bf-289e-4238-89f7-c8c7cfa76110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of test dataset: 0.692154915590864\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy of test dataset:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkNHQw2FYt74"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}